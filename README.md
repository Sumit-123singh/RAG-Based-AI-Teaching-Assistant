# RAG-Based-AI-Teaching-Assistant
"An AI-powered teaching assistant that uses Retrieval-Augmented Generation (RAG) to process video lectures, convert them into transcripts, generate embeddings, and provide accurate answers to user queries."

ğŸš€ Features

Convert video lectures to searchable text
Generate embeddings for semantic search
Interactive Q&A with your educational content
Easy-to-use pipeline for processing your data
Customizable prompt engineering

âš™ï¸ Installation & Setup

### 1ï¸âƒ£ Clone the repo

git clone https://github.com/Sumit-123singh/RAG-Based-AI-Teaching-Assistant
cd rag-teaching-assistant


2ï¸âƒ£ Install dependencies

We recommend using virtualenv or conda.
pip install -r requirements.txt
Main packages:

numpy
pandas
scikit-learn
joblib
requests

3ï¸âƒ£ Install & run Ollama

Download and install Ollama.
Pull the required models:

ollama pull bge-m3     # For embeddings
ollama pull llama3.2   # For answering queries

ğŸ› ï¸ Steps to Use (Complete Workflow)

Step 1 - Collect your videos
Move all your video files into the videos/ folder.

Step 2 - Convert to MP3

Convert all the video files to MP3:
python process_video.py
All MP3 files will be saved inside audios/.

Step 3 - Convert MP3 to JSON

Convert all the MP3 files into transcript JSON:
python sts.py
This will generate JSON transcript chunks in the jsons/ folder.

Step 4 - Convert JSON files to Vectors

Generate embeddings and save them into a joblib file:
python preprocess_json.py
This will create embedding.joblib.

Step 5 - Prompt generation & Query the Assistant

Load embeddings, create a prompt, and feed it into the LLM:
python read_chunks.py
Now you can ask any question about your videos, and the AI will retrieve the correct timestamps and give a human-like answer.

ğŸ§‘â€ğŸ« Example Usage

Ask a Question: What are curated data science packages?
Output:
ğŸ” Retrieved Context:
Video: 1_1. Installing Anaconda
Time: 79s â€“ 81s
Text: "You get to see around 300 curated data science packages."

ğŸ’¡ Final Answer:
Around 300 curated data science packages are included. You can review this at timestamp 79s â€“ 81s in "Installing Anaconda".


ğŸ“‚ Project File Structure

```text
RAG-Based Project/
â”‚
â”œâ”€â”€ venv/ # Virtual environment
â”œâ”€â”€ audios/ # MP3 files from videos
â”œâ”€â”€ videos/ # Original video lectures
â”œâ”€â”€ jsons/ # Transcripts in JSON
â”‚
â”œâ”€â”€ output.json # Consolidated JSON output
â”œâ”€â”€ embedding.joblib # Embeddings for retrieval
â”‚
â”œâ”€â”€ process_video.py # Convert videos â†’ MP3
â”œâ”€â”€ process_incoming.py # Handle user queries
â”œâ”€â”€ read_chunks.py # RAG pipeline
â”œâ”€â”€ sts.py # Speech-to-text conversion
â”‚
â”œâ”€â”€ prompt.txt # Last prompt
â”œâ”€â”€ response.txt # Last model response
â”œâ”€â”€ requirements.txt # Python dependencies
â””â”€â”€ README.md # Project documentation
```

ğŸ“Œ Notes

Ensure Ollama server is running before use:
ollama serve
Modify read_chunks.py to adjust the number of top retrieved results (top_results).
This project works completely offline once the required models are downloaded.

âš¡ Testing note: Initially, I tried processing 20 videos, but my laptop struggled due to heating (GPU temperature reached ~70Â°C) and it took ~3 hours to load all chunks.
For smoother testing, I used only 3 videos, and the pipeline worked perfectly.
Depending on your hardware specifications, you can scale up the number of videos accordingly.

## ğŸš€ Future Enhancements

1. ğŸ” **Advanced Retrieval** â€“ Use FAISS/ChromaDB for faster and more accurate semantic search.  
2. ğŸ“ **Summarization & Notes** â€“ Auto-generate concise video summaries, key points, and quizzes for quick revision. 3. ğŸ–¥ï¸ **User-Friendly GUI** â€“ Provide a smooth interface (Streamlit/Gradio) where users can upload videos, view transcripts, and interact in a chat-style Q&A.  
4. ğŸ¤– **GPT-5 Integration** â€“ Add support for GPT-5 (and other top LLMs) for more accurate and context-aware answers.  


ğŸ‘¨â€ğŸ’» Author

Sumit Singh  
AI/ML Enthusiast | Backend developer




