# RAG-Based-AI-Teaching-Assistant
"An AI-powered teaching assistant that uses Retrieval-Augmented Generation (RAG) to process video lectures, convert them into transcripts, generate embeddings, and provide accurate answers to user queries."

ğŸš€ Features

Convert video lectures to searchable text
Generate embeddings for semantic search
Interactive Q&A with your educational content
Easy-to-use pipeline for processing your data
Customizable prompt engineering

âš™ï¸ Installation & Setup

### 1ï¸âƒ£ Clone the repo

git clone https://github.com/yourusername/rag-teaching-assistant.git
cd rag-teaching-assistant


2ï¸âƒ£ Install dependencies

We recommend using virtualenv or conda.
pip install -r requirements.txt
Main packages:

numpy
pandas
scikit-learn
joblib
requests

3ï¸âƒ£ Install & run Ollama

Download and install Ollama.
Pull the required models:

ollama pull bge-m3     # For embeddings
ollama pull llama3.2   # For answering queries

ğŸ› ï¸ Steps to Use (Complete Workflow)

Step 1 - Collect your videos
Move all your video files into the videos/ folder.

Step 2 - Convert to MP3

Convert all the video files to MP3:
python process_video.py
All MP3 files will be saved inside audios/.

Step 3 - Convert MP3 to JSON

Convert all the MP3 files into transcript JSON:
python sts.py
This will generate JSON transcript chunks in the jsons/ folder.

Step 4 - Convert JSON files to Vectors

Generate embeddings and save them into a joblib file:
python preprocess_json.py
This will create embedding.joblib.

Step 5 - Prompt generation & Query the Assistant

Load embeddings, create a prompt, and feed it into the LLM:
python read_chunks.py
Now you can ask any question about your videos, and the AI will retrieve the correct timestamps and give a human-like answer.

ğŸ§‘â€ğŸ« Example Usage

Ask a Question: What are curated data science packages?
Output:
ğŸ” Retrieved Context:
Video: 1_1. Installing Anaconda
Time: 79s â€“ 81s
Text: "You get to see around 300 curated data science packages."

ğŸ’¡ Final Answer:
Around 300 curated data science packages are included. You can review this at timestamp 79s â€“ 81s in "Installing Anaconda".


ğŸ“‚ Project File Structure

'''text
RAG-Based Project/
â”‚
â”œâ”€â”€ venv/                   # ğŸ Virtual environment for dependencies
â”‚
â”œâ”€â”€ audios/                 # ğŸµ Extracted audio files (MP3 format)
â”œâ”€â”€ videos/                 # ğŸ¥ Original video lectures (input source)
â”œâ”€â”€ jsons/                  # ğŸ“‘ JSON chunks after speech-to-text (transcripts)
â”‚
â”œâ”€â”€ output.json             # Consolidated JSON output (optional)
â”œâ”€â”€ embedding.joblib        # Vector embeddings stored for retrieval
â”‚
â”œâ”€â”€ process_video.py        # ğŸ¥ Converts videos â†’ MP3 (audio extraction)
â”œâ”€â”€ process_incoming.py     # ğŸ“¨ Handles incoming user queries
â”œâ”€â”€ read_chunks.py          # ğŸ” Main RAG pipeline (retrieval + LLM response)
â”œâ”€â”€ sts.py                  # ğŸ—£ï¸ Speech-to-text pipeline (MP3 â†’ JSON)
â”‚
â”œâ”€â”€ prompt.txt              # ğŸ“ Last generated prompt (for debugging/inspection)
â”œâ”€â”€ response.txt            # ğŸ’¡ Last model response (LLM answer)
â”œâ”€â”€ requirements.txt        # ğŸ“¦ Python dependencies for the project
â”œâ”€â”€ README.md               # ğŸ“˜ Project documentation
'''


ğŸ“Œ Notes

Ensure Ollama server is running before use:
ollama serve
Modify read_chunks.py to adjust the number of top retrieved results (top_results).
This project works completely offline once the required models are downloaded.

âš¡ Testing note: Initially, I tried processing 20 videos, but my laptop struggled due to heating (GPU temperature reached ~70Â°C) and it took ~3 hours to load all chunks.
For smoother testing, I used only 3 videos, and the pipeline worked perfectly.
Depending on your hardware specifications, you can scale up the number of videos accordingly.

## ğŸš€ Future Enhancements

1. ğŸ” **Advanced Retrieval** â€“ Use FAISS/ChromaDB for faster and more accurate semantic search.  
2. ğŸ“ **Summarization & Notes** â€“ Auto-generate concise video summaries, key points, and quizzes for quick revision. 3. ğŸ–¥ï¸ **User-Friendly GUI** â€“ Provide a smooth interface (Streamlit/Gradio) where users can upload videos, view transcripts, and interact in a chat-style Q&A.  
4. ğŸ¤– **GPT-5 Integration** â€“ Add support for GPT-5 (and other top LLMs) for more accurate and context-aware answers.  


ğŸ‘¨â€ğŸ’» Author

Sumit Singh  
AI/ML Enthusiast | Backend developer




